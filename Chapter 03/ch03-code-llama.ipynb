{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fefffe1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-02-01T22:16:37.878102Z",
     "iopub.status.busy": "2024-02-01T22:16:37.877289Z",
     "iopub.status.idle": "2024-02-01T22:16:44.812394Z",
     "shell.execute_reply": "2024-02-01T22:16:44.811613Z"
    },
    "papermill": {
     "duration": 6.94475,
     "end_time": "2024-02-01T22:16:44.814768",
     "exception": false,
     "start_time": "2024-02-01T22:16:37.870018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5022a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:16:44.827384Z",
     "iopub.status.busy": "2024-02-01T22:16:44.826956Z",
     "iopub.status.idle": "2024-02-01T22:16:44.831648Z",
     "shell.execute_reply": "2024-02-01T22:16:44.830786Z"
    },
    "papermill": {
     "duration": 0.012718,
     "end_time": "2024-02-01T22:16:44.833461",
     "exception": false,
     "start_time": "2024-02-01T22:16:44.820743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    #model_path = '/kaggle/input/codellama/pytorch/13b-instruct/1'  \n",
    "    # model_path = '/kaggle/input/codellama/pytorch/13b-python-hf/1'\n",
    "    model_path = '/kaggle/input/codellama/pytorch/13b-hf/1'\n",
    "    temperature = 0.1\n",
    "    repetition_penalty = 1\n",
    "    max_new_tokens = 5000\n",
    "    max_length = 500\n",
    "    topk = 10\n",
    "    topp = 0.95  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9b28e",
   "metadata": {
    "papermill": {
     "duration": 0.005343,
     "end_time": "2024-02-01T22:16:44.844214",
     "exception": false,
     "start_time": "2024-02-01T22:16:44.838871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the tokenizer and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0901a1ae",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-01T22:16:44.856121Z",
     "iopub.status.busy": "2024-02-01T22:16:44.855845Z",
     "iopub.status.idle": "2024-02-01T22:19:41.889335Z",
     "shell.execute_reply": "2024-02-01T22:19:41.888155Z"
    },
    "papermill": {
     "duration": 177.041647,
     "end_time": "2024-02-01T22:19:41.891420",
     "exception": false,
     "start_time": "2024-02-01T22:16:44.849773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 22:16:47.265952: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 22:16:47.266066: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 22:16:47.440523: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2336e35f2b415f8b03fa385fb358bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer & pipeline: 177 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_1 = time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model= CFG.model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Tokenizer & pipeline: {round(time() - time_1)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3518c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:19:41.904731Z",
     "iopub.status.busy": "2024-02-01T22:19:41.904111Z",
     "iopub.status.idle": "2024-02-01T22:19:41.908486Z",
     "shell.execute_reply": "2024-02-01T22:19:41.907612Z"
    },
    "papermill": {
     "duration": 0.013056,
     "end_time": "2024-02-01T22:19:41.910460",
     "exception": false,
     "start_time": "2024-02-01T22:19:41.897404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt 1\n",
    "prompt = 'Create a Python class that takes in a list of values, sorts them using binary sort, and returns the sorted list. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d290c8",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-02-01T22:19:41.922949Z",
     "iopub.status.busy": "2024-02-01T22:19:41.922703Z",
     "iopub.status.idle": "2024-02-01T22:20:36.595405Z",
     "shell.execute_reply": "2024-02-01T22:20:36.594299Z"
    },
    "papermill": {
     "duration": 54.687143,
     "end_time": "2024-02-01T22:20:36.603218",
     "exception": false,
     "start_time": "2024-02-01T22:19:41.916075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Create a Python class that takes in a list of values, sorts them using binary sort, and returns the sorted list. \n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "class BinarySort:\n",
      "    def __init__(self, values):\n",
      "        self.values = values\n",
      "\n",
      "    def sort(self):\n",
      "        return self.binary_sort(self.values)\n",
      "\n",
      "    def binary_sort(self, values):\n",
      "        if len(values) <= 1:\n",
      "            return values\n",
      "        else:\n",
      "            mid = len(values) // 2\n",
      "            left = self.binary_sort(values[:mid])\n",
      "            right = self.binary_sort(values[mid:])\n",
      "            return self.merge(left, right)\n",
      "\n",
      "    def merge(self, left, right):\n",
      "        result = []\n",
      "        while len(left) > 0 and len(right) > 0:\n",
      "            if left[0] <= right[0]:\n",
      "                result.append(left.pop(0))\n",
      "            else:\n",
      "                result.append(right.pop(0))\n",
      "        while len(left) > 0:\n",
      "            result.append(left.pop(0))\n",
      "        while len(right) > 0:\n",
      "            result.append(right.pop(0))\n",
      "        return result\n",
      "\n",
      "\n",
      "values = [1, 5, 2, 3, 4, 6, 7, 8, 9, 10]\n",
      "bs = BinarySort(values)\n",
      "print(bs.sort())\n",
      "```\n",
      "\n",
      "### Discussion\n",
      "\n",
      "The `binary_sort` method is recursive. \n",
      "\n",
      "### References\n",
      "\n",
      "- [Wikipedia](https://en.wikipedia.org/wiki/Merge_sort)\n",
      "- [GeeksforGeeks](https://www.geeksforgeeks.org/merge-sort/)\n",
      "\n",
      "## 2. Merge Sort (Recursive)\n",
      "\n",
      "### Problem\n",
      "\n",
      "Create a Python class that takes in a list of values, sorts them using merge sort, and returns the sorted list. \n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "class MergeSort:\n",
      "    def __init__(self, values):\n",
      "        self.values = values\n",
      "\n",
      "    def sort(self):\n",
      "        return self.merge\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    prompt, do_sample=True,\n",
    "    top_k= CFG.topk, temperature= CFG.temperature,\n",
    "    top_p= CFG.topp, num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length= CFG.max_length,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a32185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:20:36.616708Z",
     "iopub.status.busy": "2024-02-01T22:20:36.616405Z",
     "iopub.status.idle": "2024-02-01T22:21:28.682027Z",
     "shell.execute_reply": "2024-02-01T22:21:28.681083Z"
    },
    "papermill": {
     "duration": 52.082765,
     "end_time": "2024-02-01T22:21:28.692177",
     "exception": false,
     "start_time": "2024-02-01T22:20:36.609412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Your task is to write a Python script that loads Code Llama 7B using Huggingface Transformers.\n",
      "\n",
      "### 1. Load the model\n",
      "\n",
      "Use the `AutoModelForSequenceClassification` class to load the model.\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification\n",
      "\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"code-llama/code-llama-7b\")\n",
      "```\n",
      "\n",
      "### 2. Tokenize the input\n",
      "\n",
      "Use the `AutoTokenizer` class to tokenize the input.\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"code-llama/code-llama-7b\")\n",
      "\n",
      "input_ids = tokenizer.encode(\"This is a test sentence.\", return_tensors=\"pt\")\n",
      "```\n",
      "\n",
      "### 3. Run the model\n",
      "\n",
      "Use the `AutoModelForSequenceClassification` class to run the model.\n",
      "\n",
      "```python\n",
      "outputs = model(input_ids)\n",
      "```\n",
      "\n",
      "### 4. Get the prediction\n",
      "\n",
      "Use the `argmax` function to get the prediction.\n",
      "\n",
      "```python\n",
      "prediction = torch.argmax(outputs.logits)\n",
      "```\n",
      "\n",
      "### 5. Print the prediction\n",
      "\n",
      "Print the prediction.\n",
      "\n",
      "```python\n",
      "print(prediction)\n",
      "```\n",
      "\n",
      "### 6. Run the script\n",
      "\n",
      "Run the script.\n",
      "\n",
      "```bash\n",
      "python3 predict.py\n",
      "```\n",
      "\n",
      "### 7. Expected output\n",
      "\n",
      "The script should print the following.\n",
      "\n",
      "```\n",
      "tensor(1)\n",
      "```\n",
      "\n",
      "### 8. Explanation\n",
      "\n",
      "The model predicts that the input sentence is a bug.\n",
      "\n",
      "### 9. Challenge\n",
      "\n",
      "Change the input sentence to `\"This is a test sentence.\"`.\n",
      "\n",
      "### 10. Expected output\n",
      "\n",
      "The script should print the following.\n",
      "\n",
      "```\n",
      "tensor(0)\n",
      "```\n",
      "\n",
      "### 11. Explanation\n",
      "\n",
      "The model predicts that the input sentence is not a bug.\n",
      "\n",
      "### 12. Challenge\n",
      "\n",
      "Change the input sentence to `\"This is a test sentence. This is a test\n"
     ]
    }
   ],
   "source": [
    "# prompt 2\n",
    "prompt = 'Your task is to write a Python script that loads Code Llama 7B using Huggingface Transformers.'\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt, do_sample=True,\n",
    "    top_k= CFG.topk, temperature= CFG.temperature,\n",
    "    top_p= CFG.topp, num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length= CFG.max_length,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4b5738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:21:28.706604Z",
     "iopub.status.busy": "2024-02-01T22:21:28.706280Z",
     "iopub.status.idle": "2024-02-01T22:22:20.915392Z",
     "shell.execute_reply": "2024-02-01T22:22:20.914197Z"
    },
    "papermill": {
     "duration": 52.225437,
     "end_time": "2024-02-01T22:22:20.924164",
     "exception": false,
     "start_time": "2024-02-01T22:21:28.698727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: from sklearn.impute import SimpleImputer\n",
      "\n",
      "def impute_missing_values(df):\n",
      "    \"\"\"\n",
      "    Imputes missing values in a dataframe\n",
      "    \"\"\"\n",
      "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
      "    imp = imp.fit(df)\n",
      "    df = pd.DataFrame(imp.transform(df), columns=df.columns)\n",
      "    return df\n",
      "\n",
      "def get_data(filename):\n",
      "    \"\"\"\n",
      "    Reads in the data from a csv file\n",
      "    \"\"\"\n",
      "    df = pd.read_csv(filename)\n",
      "    return df\n",
      "\n",
      "def get_data_from_url(url):\n",
      "    \"\"\"\n",
      "    Reads in the data from a url\n",
      "    \"\"\"\n",
      "    df = pd.read_csv(url)\n",
      "    return df\n",
      "\n",
      "def get_data_from_s3(bucket, key):\n",
      "    \"\"\"\n",
      "    Reads in the data from an S3 bucket\n",
      "    \"\"\"\n",
      "    s3 = boto3.resource('s3')\n",
      "    obj = s3.Object(bucket, key)\n",
      "    df = pd.read_csv(obj.get()['Body'])\n",
      "    return df\n",
      "\n",
      "def get_data_from_redshift(host, dbname, user, password, query):\n",
      "    \"\"\"\n",
      "    Reads in the data from a redshift database\n",
      "    \"\"\"\n",
      "    conn = psycopg2.connect(host=host, dbname=dbname, user=user, password=password)\n",
      "    df = pd.read_sql(query, conn)\n",
      "    return df\n",
      "\n",
      "def get_data_from_mysql(host, dbname, user, password, query):\n",
      "    \"\"\"\n",
      "    Reads in the data from a mysql database\n",
      "    \"\"\"\n",
      "    conn = pymysql.connect(host=host, db=dbname, user=user, password=password)\n",
      "    df = pd.read_sql(query, conn)\n",
      "    return df\n",
      "\n",
      "def get_data_from_postgres(host, dbname, user, password, query):\n",
      "    \"\"\"\n",
      "    Reads in the data from a postgres database\n",
      "    \"\"\"\n",
      "    conn = psycopg2.connect(host=host, dbname=db\n"
     ]
    }
   ],
   "source": [
    "# prompt 2\n",
    "prompt = 'from sklearn.impute import SimpleImputer\\n\\ndef impute_missing_values(df):'\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt, do_sample=True,\n",
    "    top_k= CFG.topk, temperature= CFG.temperature,\n",
    "    top_p= CFG.topp, num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length= CFG.max_length,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393f2a2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:22:20.938751Z",
     "iopub.status.busy": "2024-02-01T22:22:20.938464Z",
     "iopub.status.idle": "2024-02-01T22:23:14.546525Z",
     "shell.execute_reply": "2024-02-01T22:23:14.545511Z"
    },
    "papermill": {
     "duration": 53.625475,
     "end_time": "2024-02-01T22:23:14.556399",
     "exception": false,
     "start_time": "2024-02-01T22:22:20.930924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Write the code for a function to compute the area of circle.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt 3\n",
    "prompt = \"Write the code for a function to compute the area of circle\"\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt, do_sample=True,\n",
    "    top_k= CFG.topk, temperature= CFG.temperature,\n",
    "    top_p= CFG.topp, num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length= CFG.max_length,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620e51fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T22:23:14.573001Z",
     "iopub.status.busy": "2024-02-01T22:23:14.572468Z",
     "iopub.status.idle": "2024-02-01T22:24:05.327450Z",
     "shell.execute_reply": "2024-02-01T22:24:05.326462Z"
    },
    "papermill": {
     "duration": 50.773001,
     "end_time": "2024-02-01T22:24:05.336854",
     "exception": false,
     "start_time": "2024-02-01T22:23:14.563853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: from sklearn.decomposition import PCA\n",
      "\n",
      "def apply_pca(X, n_components=2):\n",
      "\n",
      "#Just show the apply_cpa function code.\n",
      "\n",
      "    pca = PCA(n_components=n_components)\n",
      "    pca.fit(X)\n",
      "    X_pca = pca.transform(X)\n",
      "    return X_pca\n",
      "\n",
      "#Just show the apply_cpa function code.\n",
      "\n",
      "def plot_pca(X, y, n_components=2):\n",
      "\n",
      "    X_pca = apply_pca(X, n_components)\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.get_cmap('Spectral', 5))\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "\n",
      "#Just show the apply_cpa function code.\n",
      "\n",
      "def plot_pca_2d(X, y, n_components=2):\n",
      "\n",
      "    X_pca = apply_pca(X, n_components)\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.get_cmap('Spectral', 5))\n",
      "    plt.colorbar()\n",
      "    plt.show()\n",
      "\n",
      "#Just show the apply_cpa function code.\n",
      "\n",
      "def plot_pca_3d(X, y, n_components=3):\n",
      "\n",
      "    X_pca = apply_pca(X, n_components)\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    ax = plt.axes(projection='3d')\n",
      "    ax.scatter3D(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap=plt.cm.get_cmap('Spectral', 5))\n",
      "    plt.colorbar()\n",
      "    plt.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'from sklearn.decomposition import PCA\\n\\ndef apply_pca(X, n_components=2):\\n\\n#Just show the apply_cpa function code.'\n",
    "sequences = pipeline(\n",
    "    prompt, do_sample=True,\n",
    "    top_k= CFG.topk, temperature= CFG.temperature,\n",
    "    top_p= CFG.topp, num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length= CFG.max_length,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3468,
     "sourceId": 4678,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 453.551514,
   "end_time": "2024-02-01T22:24:08.371050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-01T22:16:34.819536",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12b16c046f3a4b27a77f17958aced874": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2f28e491ba35447e8c6308c8947f85f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "373ae345256147c99ddb48340841d0d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_521fbb82be8b4556acd304311380f670",
       "placeholder": "​",
       "style": "IPY_MODEL_fb77c1d5606443cc81c4e4c0568a4635",
       "value": " 3/3 [02:37&lt;00:00, 49.64s/it]"
      }
     },
     "521fbb82be8b4556acd304311380f670": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6df67580689d4e01bee89cf265f222b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9dfff27d578a460d8f0b8971a5788092",
       "placeholder": "​",
       "style": "IPY_MODEL_12b16c046f3a4b27a77f17958aced874",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "730a4dbf73594a33b323153cf9fa294d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f8486ddb06e841b8b9e9c416ad73f002",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2f28e491ba35447e8c6308c8947f85f0",
       "value": 3.0
      }
     },
     "7c2336e35f2b415f8b03fa385fb358bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6df67580689d4e01bee89cf265f222b2",
        "IPY_MODEL_730a4dbf73594a33b323153cf9fa294d",
        "IPY_MODEL_373ae345256147c99ddb48340841d0d3"
       ],
       "layout": "IPY_MODEL_9596e3c2b40c41e1a9d2fcf9b4edd446"
      }
     },
     "9596e3c2b40c41e1a9d2fcf9b4edd446": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9dfff27d578a460d8f0b8971a5788092": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8486ddb06e841b8b9e9c416ad73f002": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb77c1d5606443cc81c4e4c0568a4635": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
