{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install MonsterAPI pypi client"
      ],
      "metadata": {
        "id": "OTJqHSqsD_3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slr71YbqDAUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7eaa8c-dd36-4814-9c21-f21e2c1435d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monsterapi==1.0.2b3\n",
            "  Downloading monsterapi-1.0.2b3-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from monsterapi==1.0.2b3) (2.31.0)\n",
            "Collecting requests-toolbelt (from monsterapi==1.0.2b3)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from monsterapi==1.0.2b3) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->monsterapi==1.0.2b3) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->monsterapi==1.0.2b3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->monsterapi==1.0.2b3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->monsterapi==1.0.2b3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->monsterapi==1.0.2b3) (2023.7.22)\n",
            "Installing collected packages: requests-toolbelt, monsterapi\n",
            "Successfully installed monsterapi-1.0.2b3 requests-toolbelt-1.0.0\n"
          ]
        }
      ],
      "source": [
        "# Please install specific beta version of client for quick serve access.\n",
        "!python3 -m pip install monsterapi==1.0.2b3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sign up on [MonsterAPI](https://monsterapi.ai/signup?utm_source=llama-index-colab&utm_medium=referral) and get a free auth key. Paste it below:\n",
        "Make sure you have signed up  for beta access at [here](https://forms.gle/TTJRapHm59RxjttJA)"
      ],
      "metadata": {
        "id": "4qB0zjJLEzrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"<replace with your api key>\""
      ],
      "metadata": {
        "id": "5TNL3HjsEzYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize client"
      ],
      "metadata": {
        "id": "z4cDy6EgEEuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from monsterapi import client as mclient\n",
        "deploy_client = mclient(api_key = api_key)"
      ],
      "metadata": {
        "id": "YWwbfgYKEHdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch a LLM QuickServe deployment\n",
        "Let quick serve Code LLama with lora adapter from hugging face. Craeate a deployment on a 24GB RTXA5000 GPU.\n",
        "This RestAPI can serve 380 Tokens/Sec through REST API with both static and streaming response support.\n",
        "\n",
        "```\n",
        "    basemodel_path=\"codellama/CodeLlama-7b-hf\"\n",
        "    loramodel_path=\"qblocks/codellama7b_codealpaca20k\"\n",
        "    prompt_template=\"{prompt}{completion}\"\n",
        "    per_gpu_vram=24\n",
        "    gpu_count=1\n",
        "```"
      ],
      "metadata": {
        "id": "hpIG5bHtEPlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "launch_payload = {\n",
        "    \"basemodel_path\": \"codellama/CodeLlama-7b-hf\",\n",
        "    \"loramodel_path\": \"https://finetuning-service.s3.us-east-2.amazonaws.com/finetune_outputs/2866f6ea-1427-416a-a043-1b573f404587/2866f6ea-1427-416a-a043-1b573f404587.zip\",\n",
        "    \"prompt_template\": \"{prompt}{completion}\",\n",
        "    \"api_auth_token\": \"b6a97d3b-35d0-4720-a44c-59ee33dbc25b\",\n",
        "    \"per_gpu_vram\": 24,\n",
        "    \"gpu_count\": 1\n",
        "}\n",
        "\n",
        "# Launch a deployment\n",
        "ret = deploy_client.deploy(\"llm\", launch_payload)\n",
        "deployment_id = ret.get(\"deployment_id\")\n",
        "print(deployment_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLYZllWuEUar",
        "outputId": "a456de9b-b4cf-4130-ead7-f6fb1a5d997c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "afede594-90b3-43c8-b0eb-14fbe8d7973c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Status of deployment\n",
        "\n",
        "#### Progress of status responses:\n",
        "1. INPROGRESS:\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"pending\",\n",
        "        \"message\":\"Instance is still being provisioned, pls wait and try again\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "2. Fail\n",
        "    ```\n",
        "    {\n",
        "        \"status\": \"failed\",\n",
        "        \"message\": \"Instance has failed, pls launch a new instance\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "3. Building\n",
        "    ```\n",
        "    {\n",
        "        \"status\": \"building\",\n",
        "        \"message\": \"Server has started but trying to connect to deployment container, just downloading your model and setting things up, please try again in few mins, if state persists, please use /restart or /terminate!\"\n",
        "    }\n",
        "    ```\n",
        "4. Live\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"live\",\n",
        "        \"message\":\"Server has started !!!\",\n",
        "        \"URL\":\"http://jus.qblocks.cloud:58744\",\n",
        "        \"api_auth_token\":\"57b7b903-a4b6-4720-8154-af71aa8e8313\"\n",
        "    }\n",
        "    visit the url to get the llm service endpoint details or above url/docs to get swagger docs\n",
        "    ```\n",
        "5. Terminated by User\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"terminatedByUser\",\n",
        "        \"message\":\"Instance is terminatedByUser\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "6. Terminated by System (Out of  Credits)\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"terminatedBySystem\",\n",
        "        \"message\":\"Instance is terminatedBySystem\"\n",
        "    }\n",
        "    ```\n",
        "\n"
      ],
      "metadata": {
        "id": "w9gUNXReFPYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Status of Deployments\n"
      ],
      "metadata": {
        "id": "e-GadRntMRFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status_debug = False # Just a placeholder to show possible statuses."
      ],
      "metadata": {
        "id": "n3n6CgwFO9os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pending Status"
      ],
      "metadata": {
        "id": "7c8BFW92NCeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if status_debug:\n",
        "  status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "  print(status_ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUDPqMj6FSNs",
        "outputId": "34decaaa-2e64-407a-fa3e-db90e3eb19d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'pending', 'message': 'Instance is still being provisioned, pls wait and try again'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building status"
      ],
      "metadata": {
        "id": "WbI2iNYKNxr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if status_debug:\n",
        "  status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "  print(status_ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkawseI4NxNq",
        "outputId": "e2e62237-0cfd-49b5-b341-94ea83b3fb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'building', 'message': 'Server has started but trying to connect to deployment container, just downloading your model and setting things up, please try again in few mins, if state persists, please use /restart or /terminate!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Logs of deployment available from building status"
      ],
      "metadata": {
        "id": "5SZqKMLSFWWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs_ret = deploy_client.get_deployment_logs(deployment_id, n_lines = 50)\n",
        "if 'logs' not in logs_ret:\n",
        "  raise Exception(\"Please wait until status changes to building!\")\n",
        "for i in logs_ret['logs']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-stvBsoFV0p",
        "outputId": "4b65aafe-d53f-4d93-aa87-82deda8eab8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please make sure that you have a GPU available and that you have installed the correct CUDA drivers.\n",
            "No CUDA GPUs are available\n",
            "Please make sure that you have a GPU available and that you have installed the correct CUDA drivers.\n",
            "No CUDA GPUs are available\n",
            "Please make sure that you have a GPU available and that you have installed the correct CUDA drivers.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "\n",
            "=============\n",
            "== PyTorch ==\n",
            "=============\n",
            "\n",
            "NVIDIA Release 22.12 (build 49968248)\n",
            "PyTorch Version 1.14.0a0+410ce96\n",
            "\n",
            "Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
            "\n",
            "Copyright (c) 2014-2022 Facebook Inc.\n",
            "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n",
            "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n",
            "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n",
            "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n",
            "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n",
            "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n",
            "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n",
            "Copyright (c) 2015      Google Inc.\n",
            "Copyright (c) 2015      Yangqing Jia\n",
            "Copyright (c) 2013-2016 The Caffe contributors\n",
            "All rights reserved.\n",
            "\n",
            "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
            "\n",
            "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
            "By pulling and using the container, you accept the terms and conditions of this license:\n",
            "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
            "\n",
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.54it/s]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.70it/s]\n",
            "Using pad_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "INFO 11-28 08:46:59 llm_engine.py:72] Initializing an LLM engine with config: model='model', tokenizer='model', tokenizer_mode=auto, revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "INFO 11-28 08:47:14 llm_engine.py:205] # GPU blocks: 1062, # CPU blocks: 512\n",
            "INFO:     Started server process [1]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Live Status"
      ],
      "metadata": {
        "id": "ooGyyGeYNJo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9cvSEUEM-44",
        "outputId": "dda9acf3-d068-43d5-dc02-535153e0c8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'live', 'message': 'Server has started !!!', 'URL': 'https://216.153.51.213', 'api_auth_token': 'b6a97d3b-35d0-4720-a44c-59ee33dbc25b'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spinning up the instance take few mins! We are working on optimizing the service. 'status' will set to `building` and then to `live` as build progresses and logs will be available from 'building' state."
      ],
      "metadata": {
        "id": "zFGFlRGDGEZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Once the deployment is live, lets see how we can use the quick serve llm endpoint\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "x9x1-SGvG986"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "assert status_ret.get(\"status\") == \"live\", \"Please wait until status is live!\"\n",
        "\n",
        "service_client  = mclient(api_key = status_ret.get(\"api_auth_token\"), base_url = status_ret.get(\"URL\"))\n",
        "\n",
        "payload = {\n",
        "    \"input_variables\": {\n",
        "        \"prompt\": \"What is Moore's law ?\"},\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.6,\n",
        "    \"max_tokens\": 512\n",
        "}\n",
        "\n",
        "output = service_client.generate(model = \"deploy-llm\", data = payload)\n",
        "\n",
        "if payload.get(\"stream\"):\n",
        "    for i in output:\n",
        "        print(i[0])\n",
        "else:\n",
        "    print(json.loads(output)['text'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo8e194fG7eA",
        "outputId": "3737702b-c1f0-4583-998e-6c49b85ec3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host '216.153.51.213'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Moore's law ?\n",
            "Below is the definition of Moore's law from Wikipedia:\n",
            "Moore's law is a prediction made by Gordon E. Moore in 1965 that the transistor density of integrated circuits would double every two years. The law predicts that the number of transistors per unit area in integrated circuits will double roughly every two years. The law was first formulated in 1965.\n",
            "How is Moore's law used in practice?\n",
            "Moore's law is used to estimate the time required to complete a task. It can be used to determine the time required to complete a task by estimating the number of transistors that need to be used in a given circuit. For example, if the number of transistors used in a circuit is doubled every two years, then to complete a task in two years, a circuit with 1 million transistors will take approximately 4 years to complete. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terminate Deployment"
      ],
      "metadata": {
        "id": "04II1U-EFoHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terminate_return = deploy_client.terminate_deployment(deployment_id)\n",
        "print(terminate_return)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOyja-vFqU8",
        "outputId": "a56572f7-9638-40f2-8d33-e04401b333a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'message': 'Instance Terminated'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Terminate Status"
      ],
      "metadata": {
        "id": "mN_aby3kROkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Li7xJuTRMjL",
        "outputId": "45cd8f68-b0e6-45d7-9e21-21ec947b3762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'status': 'terminatedByUser', 'message': 'Instance is terminatedByUser'}\n"
          ]
        }
      ]
    }
  ]
}